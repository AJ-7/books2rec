{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import math\n",
    "import random\n",
    "from scipy import sparse\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel, cosine_similarity\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from surprise import Reader, Dataset, SVD, evaluate, dump, accuracy\n",
    "from collections import defaultdict\n",
    "\n",
    "# Custom libraries\n",
    "sys.path.append('../Util')\n",
    "from loader import get_books, get_book_dataframe, get_book_features\n",
    "from joiner import get_ratings, get_joint, load_amazon, load_goodreads\n",
    "from reduction import reduce_matrix, get_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(user_to_concept, V, user_bias, global_bias):\n",
    "    pred_ratings = np.zeros(len(V))\n",
    "    for i in range(len(V)):\n",
    "        pred = global_bias + new_user_bias + np.dot(new_user_P, qi[i])\n",
    "        pred_ratings[i] = pred\n",
    "    return pred_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_recs(result, books, n, q):\n",
    "    recs = []\n",
    "    for i in range(len(result)):\n",
    "        if q[i] == 0: # book user hasn't already rated\n",
    "            recs.append((i, result[i]))\n",
    "        else:\n",
    "            recs.append((i, float('-inf'))) \n",
    "            # recs.append((i, result[i])) #leave this to verify things actually working\n",
    "    recs = sorted(recs, key=lambda tup: tup[1], reverse=True)\n",
    "\n",
    "    top_titles = []\n",
    "    for i in range(n):\n",
    "        book_id = recs[i][0]\n",
    "        title = books.iloc[book_id]['title']\n",
    "        top_titles.append(title)\n",
    "    return top_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set this to where you save and load all data\n",
    "data_path = '../../goodbooks-10k/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found books_dataframe in file...\n"
     ]
    }
   ],
   "source": [
    "# Get dataframe from books\n",
    "books = get_book_dataframe(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '../.tmp/svd_100_300.npy'\n",
    "qi = np.load(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 300)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qi.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 3, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# user from goodreads\n",
    "# sparse_q = scipy.sparse.load_npz('../.tmp/cached_users/user_likes_mystery_scifi_hates_fantasy.npz')\n",
    "# sparse_q = scipy.sparse.load_npz('../.tmp/cached_users/user_likes_fantasy.npz')\n",
    "sparse_q = scipy.sparse.load_npz('../.tmp/cached_users/user_nickgreenquist.npz')\n",
    "q = sparse_q.toarray()\n",
    "q = np.array(q[0].tolist())\n",
    "new_user = np.copy(q)\n",
    "new_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 5, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# undo the rating mapping we usually do\n",
    "\n",
    "# Turn 1-5 rating scale into negative - positive scale\n",
    "# original mapper: ratings_mapper = {0:0, 1:-2, 2:-1, 3:1, 4:2, 5:3}\n",
    "ratings_mapper = {0:0, -2:-1, -1:-2, 1:3, 2:4, 3:5}\n",
    "for i in range(len(q)):\n",
    "    new_user[i] = ratings_mapper[new_user[i]]\n",
    "new_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create array of indices of books this user has actually rated\n",
    "indices = []\n",
    "for i in range(len(new_user)):\n",
    "    if new_user[i] != 0:\n",
    "        indices.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparams\n",
    "learning_rate = 0.01\n",
    "user_bias_reg = 0.1\n",
    "P_reg = 0.001\n",
    "global_bias = 3.946136\n",
    "\n",
    "# 25 updates per rated book\n",
    "iterations = len(indices) * 25\n",
    "\n",
    " # see total loss 10 times\n",
    "show_total_loss = iterations / 10\n",
    "\n",
    "n_factors = qi.shape[1]\n",
    "cols = qi.shape[0]\n",
    "\n",
    "# TODO: save item biases after training with Surprise\n",
    "item_bias = np.full(10000, 4.0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. get the user_bias for this user\n",
    "new_user_bias = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. set up new random P\n",
    "mu, sigma = 0, 0.1\n",
    "new_user_P = np.random.normal(mu, (sigma / n_factors), n_factors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at Iteration 0: 249.57045934045692\n",
      "Loss at Iteration 505: 215.11055207583487\n",
      "Loss at Iteration 1010: 223.4407353010497\n",
      "Loss at Iteration 1515: 200.37341456146817\n",
      "Loss at Iteration 2020: 183.26329867905932\n",
      "Loss at Iteration 2525: 184.96313958213838\n",
      "Loss at Iteration 3030: 191.17734120180992\n",
      "Loss at Iteration 3535: 173.8658084082772\n",
      "Loss at Iteration 4040: 178.84108342962801\n",
      "Loss at Iteration 4545: 167.36259446980685\n"
     ]
    }
   ],
   "source": [
    "# 3. computer small number of iterations of SGD\n",
    "for iteration in range(iterations):\n",
    "    \n",
    "    # TODO: is there a way to not redo ALL errors after just one item update??\n",
    "    \n",
    "    # 3.1 calculate loss with current components\n",
    "    errors = np.zeros(shape=(cols), dtype=np.float)\n",
    "    for i in indices:\n",
    "        rating = new_user[i]\n",
    "        # pred = global_bias + new_user_bias + item_bias[i] + np.dot(new_user_P, qi[i])\n",
    "        pred = new_user_bias + np.dot(new_user_P, qi[i])\n",
    "        errors[i] = rating - pred\n",
    "    \n",
    "\n",
    "    # 3.2 periodically calculate total loss and output\n",
    "    if iteration % show_total_loss == 0:\n",
    "        total_loss = 0.0\n",
    "        for j in indices:\n",
    "            total_loss += pow(errors[j], 2)\n",
    "        print(\"Loss at Iteration {}: {}\".format(iteration, total_loss))\n",
    "\n",
    "    # 3.3 run single SGD iteration\n",
    "    i = random.choice(indices)\n",
    "\n",
    "    # update P\n",
    "    for f in range(n_factors):\n",
    "        p_update = learning_rate * (errors[i] * qi[i][f] - P_reg * new_user_P[f])\n",
    "        new_user_P[f] += p_update\n",
    "\n",
    "    # update user bias\n",
    "    ub_update = learning_rate * (errors[i] - user_bias_reg * new_user_bias)\n",
    "    new_user_bias += ub_update\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bad Monkey\n",
      "Herzog\n",
      "The Hundred Thousand Kingdoms (Inheritance Trilogy, #1)\n",
      "The Drunkard's Walk: How Randomness Rules Our Lives\n",
      "Nudge: Improving Decisions About Health, Wealth, and Happiness\n",
      "ظل الأفعى\n",
      "Here's to Us\n",
      "If You Could See Me Now\n",
      "Breaking Night: A Memoir of Forgiveness, Survival, and My Journey from Homeless to Harvard\n",
      "How the Light Gets In (Chief Inspector Armand Gamache, #9)\n",
      "The Next Best Thing\n",
      "Blood Memory\n",
      "Black-Eyed Susans\n",
      "The Work of Art in the Age of Its Technological Reproducibility, and Other Writings on Media\n",
      "The Story of Babar\n",
      "Girl at War\n",
      "The Gift of the Magi\n",
      "Hogwarts: An Incomplete and Unreliable Guide (Pottermore Presents, #3)\n",
      "Millennium Snow, Vol. 1\n",
      "Hamilton: The Revolution\n",
      "Unveiled: Tamar (Lineage of Grace #1)\n",
      "Wool (Wool, #1)\n",
      "The Handmaid's Tale\n",
      "Caraval\n",
      "Acheron (Dark-Hunter #14)\n"
     ]
    }
   ],
   "source": [
    "recs = get_top_n_recs(get_predictions(new_user_P, qi, new_user_bias, global_bias), books, 25, new_user)\n",
    "for r in recs:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
