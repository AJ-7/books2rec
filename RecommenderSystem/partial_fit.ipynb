{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import math\n",
    "import random\n",
    "from scipy import sparse\n",
    "from collections import defaultdict\n",
    "\n",
    "# Custom libraries\n",
    "sys.path.append('../Util')\n",
    "from loader import get_books, get_book_dataframe, get_book_features\n",
    "from joiner import get_ratings, get_joint, load_amazon, load_goodreads\n",
    "from reduction import reduce_matrix, get_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_user_to_features(p, features):\n",
    "    p_sparse = scipy.sparse.csr_matrix(p)\n",
    "    # map new user to concept space by p*features\n",
    "    user_to_concept = p_sparse.dot(features)\n",
    "    # map user back to itme space with user_to_concept * featuresT\n",
    "    result = user_to_concept.dot(features.T).todense()\n",
    "    return result.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(p, q, user_bias, item_bias, global_bias):\n",
    "    pred_ratings = np.zeros(len(q))\n",
    "    for i in range(len(q)):\n",
    "        pred = global_bias + user_bias + item_bias[i] + np.dot(p, q[i])\n",
    "        # pred = global_bias + user_bias + np.dot(p, q[i])\n",
    "        pred_ratings[i] = pred\n",
    "    return pred_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_recs(result, books, n, q):\n",
    "    recs = []\n",
    "    for i in range(len(result)):\n",
    "        if q[i] == 0: # book user hasn't already rated\n",
    "            recs.append((i, result[i]))\n",
    "        else:\n",
    "            recs.append((i, float('-inf'))) \n",
    "            # recs.append((i, result[i])) #leave this to verify things actually working\n",
    "    recs = sorted(recs, key=lambda tup: tup[1], reverse=True)\n",
    "\n",
    "    top_titles = []\n",
    "    for i in range(n):\n",
    "        book_id = recs[i][0]\n",
    "        title = books.iloc[book_id]['title']\n",
    "        top_titles.append(title)\n",
    "    return top_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set this to where you save and load all data\n",
    "data_path = '../../goodbooks-10k/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dataframe from books\n",
    "books = get_book_dataframe(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cu2rec components\n",
    "filename = '../.tmp/goodbooks_sorted_f300'\n",
    "q = genfromtxt('{}_q.csv'.format(filename), delimiter=',')\n",
    "item_bias = genfromtxt('{}_item_bias.csv'.format(filename), delimiter=',')\n",
    "\n",
    "\n",
    "# surprise components\n",
    "# filename = '../.tmp/svd_100_300.npy'\n",
    "# q = np.load(filename)\n",
    "# filename = '../.tmp/Q_300.npy'\n",
    "# q = np.load(filename)\n",
    "# filename = '../.tmp/item_bias_300.npy'\n",
    "# item_bias = np.load(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert global bias to float - get from whatever dataset you used\n",
    "global_bias = 3.919866"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user from goodreads\n",
    "# sparse_new_user_scaled = scipy.sparse.load_npz('../.tmp/cached_users/user_likes_mystery_scifi_hates_fantasy.npz')\n",
    "# sparse_new_user_scaled = scipy.sparse.load_npz('../.tmp/cached_users/user_likes_fantasy.npz')\n",
    "sparse_new_user_scaled = scipy.sparse.load_npz('../.tmp/cached_users/user_nickgreenquist.npz')\n",
    "new_user_ratings_scaled = sparse_new_user_scaled.toarray()\n",
    "new_user_ratings_scaled = np.array(new_user_ratings_scaled[0].tolist())\n",
    "new_user_ratings = np.copy(new_user_ratings_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# undo the rating mapping we usually do\n",
    "\n",
    "# Turn 1-5 rating scale into negative - positive scale\n",
    "# original mapper: ratings_mapper = {0:0, 1:-2, 2:-1, 3:1, 4:2, 5:3}\n",
    "ratings_mapper = {0:0, -2:-1, -1:-2, 1:3, 2:4, 3:5}\n",
    "for i in range(len(q)):\n",
    "    new_user_ratings[i] = ratings_mapper[new_user_ratings_scaled[i]]\n",
    "new_user_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create array of indices of books this user has actually rated\n",
    "indices = []\n",
    "for i in range(len(new_user_ratings)):\n",
    "    if new_user_ratings[i] != 0:\n",
    "        indices.append(i)\n",
    "len(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparams\n",
    "learning_rate = 0.07\n",
    "user_bias_reg = 0.002\n",
    "P_reg = 0.002\n",
    "\n",
    "# updates per rating\n",
    "iterations = len(indices) * 20\n",
    "\n",
    " # how many iterations to see the total loss at this step - remove in webapp!\n",
    "calculate_total_loss = float('inf')\n",
    "\n",
    "n_factors = q.shape[1]\n",
    "cols = q.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. set the user_bias for this user\n",
    "new_user_bias = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. set up new random P\n",
    "mu, sigma = 0, 0.1\n",
    "p = np.random.normal(mu, (sigma / n_factors), n_factors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. computer small number of iterations of SGD\n",
    "for iteration in range(iterations):\n",
    "    \n",
    "    #= periodically calculate total loss and output\n",
    "    if iteration == 0 or iteration == iterations - 1 or iteration % calculate_total_loss == 0:\n",
    "        total_loss = 0.0\n",
    "        for i in indices:\n",
    "            rating = new_user_ratings[i]\n",
    "            pred = global_bias + new_user_bias + item_bias[i] + np.dot(p, q[i])\n",
    "            # pred = global_bias + new_user_bias + np.dot(p, q[i])\n",
    "            error = rating - pred\n",
    "            total_loss += pow(error, 2)\n",
    "            \n",
    "        rmse = math.sqrt(total_loss / len(indices))\n",
    "        print(\"RMSE at Iteration {}: {}\".format(iteration, rmse))\n",
    "\n",
    "    # Gradient Descent using every book - ucomment below to go back to SGD\n",
    "    i = random.choice(indices)\n",
    "#     for i in indices:\n",
    "    \n",
    "    # calculate loss on random item\n",
    "    rating = new_user_ratings[i]\n",
    "    pred = global_bias + new_user_bias + item_bias[i] + np.dot(p, q[i])\n",
    "    # pred = global_bias + new_user_bias + np.dot(p, q[i])\n",
    "    error = rating - pred\n",
    "\n",
    "    # update P\n",
    "    for f in range(n_factors):\n",
    "        p_update = learning_rate * (error * q[i][f] - P_reg * p[f])\n",
    "        p[f] += p_update\n",
    "\n",
    "    # update user bias\n",
    "    ub_update = learning_rate * (error - user_bias_reg * new_user_bias)\n",
    "    new_user_bias += ub_update\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions using partial fit\n",
    "predictions_partial_fit = get_predictions(p, q, new_user_bias, item_bias, global_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out top results using just partial fit predictions\n",
    "recs_partial_fit = get_top_n_recs(predictions_partial_fit, books, 25, new_user_ratings)\n",
    "for rec in recs_partial_fit:\n",
    "    print(rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Combine recs from partial fit with recs from mapping to feature matrix using log_rank\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# produce feature matrix\n",
    "feature_matrix = get_book_features(books)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions using feature matrix\n",
    "predictions_features = map_user_to_features(new_user_ratings, feature_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Log Ranking\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tuple of book_id and rating for each method, then sort\n",
    "partial_fit_ratings = []\n",
    "feature_ratings = []\n",
    "for i in range(len(books)):\n",
    "    partial_fit_ratings.append((i, predictions_partial_fit[i]))\n",
    "    feature_ratings.append((i, predictions_features[i]))\n",
    "\n",
    "partial_fit_ratings = sorted(partial_fit_ratings, key=lambda x: x[1], reverse=True)\n",
    "feature_ratings = sorted(feature_ratings, key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map book_id to the rank for each method\n",
    "id_to_rank_partial_fit = {}\n",
    "id_to_rank_features = {}\n",
    "for i in range(len(books)):\n",
    "    book_id = partial_fit_ratings[i][0]\n",
    "    id_to_rank_partial_fit[book_id] = math.log(i+1)\n",
    "\n",
    "    book_id = feature_ratings[i][0]\n",
    "    id_to_rank_features[book_id] = math.log(i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_feature = 0.5\n",
    "\n",
    "rankings = []\n",
    "for i in range(len(books)):\n",
    "    if new_user_ratings[i] == 0:\n",
    "        rank = weight_feature*id_to_rank_features[i] + (1.0-weight_feature)*id_to_rank_partial_fit[i]\n",
    "        rankings.append((rank, i))\n",
    "rankings = sorted(rankings, key=lambda x: x[0])\n",
    "print(len(rankings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_books = []\n",
    "for i in range(100):\n",
    "    book_id = rankings[i][1]\n",
    "    book = books.iloc[book_id] # index is book_id - 1\n",
    "    book['rank'] = i + 1\n",
    "    top_books.append(book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for book in top_books:\n",
    "    print(book['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
